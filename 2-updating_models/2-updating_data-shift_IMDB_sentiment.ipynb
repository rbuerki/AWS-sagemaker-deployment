{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Updating a Model in SageMaker\n",
    "\n",
    "_Deep Learning Nanodegree Program | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we will consider a situation in which a model that we constructed is no longer working as we intended. In particular, we will look at the XGBoost sentiment analysis model that we constructed earlier. In this case, however, we have some new data that our model doesn't seem to perform very well on. As a result, we will re-train our model and update an existing endpoint so that it uses our new model.\n",
    "\n",
    "This notebook starts by re-creating the XGBoost sentiment analysis model that was created in the deployment section. This means the cells up to the end of Step 4 are the same as before. _The new content in this notebook begins at Step 5._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2019-12-30 14:09:09--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  10.6MB/s    in 11s     \n",
      "\n",
      "2019-12-30 14:09:20 (7.43 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like this movie because it is a fine work of cinema, made by people who care enough to make it art and not just home movies. It is filled with Super-surfer Greg Noll's home movies, and a boatload of amateur video from others who align themselves with his 50-year passion. Nevertheless, it has been expanded to the degree that it approaches aesthetic glory. It is filled with artistic talent, and athletic talent, however trivial you might think surfing to be athletic. Surfers are not astronauts nor test-pilots. Nor are they surgeons(perhaps) or Ph.d's(again, perhaps). It believes in the quest of the surfer. It believes in the beauty of human goofiness. It believes in the great gift of peace, which comes from the cessation of war. Surfers celebrate the cessation of war on the north beach of an Hawaiian island attacked by Japanese zeroes fifteen years before. It celebrates the down-time of a country which fought a cold war-instead of a hot-war - with the Russian socialists. Surfing is the ultimate narcissism. It is dangerous, but only slightly historical. I suspect Alexander the Great would not be celebrated for his surfing technique. He had to go out and conquer a few dozen countries to get the favorable press he has received. This movie has no military heroes. It has no guns. The only beach-head surfers conquer has a beer-stand and and a surfboard shop. This is not a problem. Peace is not desperate. It is the joy of exhalation.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Processing the data\n",
    "\n",
    "(Alternative approach, as we have more flexibility as we had when using a lambda function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'movi',\n",
       " 'fine',\n",
       " 'work',\n",
       " 'cinema',\n",
       " 'made',\n",
       " 'peopl',\n",
       " 'care',\n",
       " 'enough',\n",
       " 'make',\n",
       " 'art',\n",
       " 'home',\n",
       " 'movi',\n",
       " 'fill',\n",
       " 'super',\n",
       " 'surfer',\n",
       " 'greg',\n",
       " 'noll',\n",
       " 'home',\n",
       " 'movi',\n",
       " 'boatload',\n",
       " 'amateur',\n",
       " 'video',\n",
       " 'other',\n",
       " 'align',\n",
       " '50',\n",
       " 'year',\n",
       " 'passion',\n",
       " 'nevertheless',\n",
       " 'expand',\n",
       " 'degre',\n",
       " 'approach',\n",
       " 'aesthet',\n",
       " 'glori',\n",
       " 'fill',\n",
       " 'artist',\n",
       " 'talent',\n",
       " 'athlet',\n",
       " 'talent',\n",
       " 'howev',\n",
       " 'trivial',\n",
       " 'might',\n",
       " 'think',\n",
       " 'surf',\n",
       " 'athlet',\n",
       " 'surfer',\n",
       " 'astronaut',\n",
       " 'test',\n",
       " 'pilot',\n",
       " 'surgeon',\n",
       " 'perhap',\n",
       " 'ph',\n",
       " 'perhap',\n",
       " 'believ',\n",
       " 'quest',\n",
       " 'surfer',\n",
       " 'believ',\n",
       " 'beauti',\n",
       " 'human',\n",
       " 'goofi',\n",
       " 'believ',\n",
       " 'great',\n",
       " 'gift',\n",
       " 'peac',\n",
       " 'come',\n",
       " 'cessat',\n",
       " 'war',\n",
       " 'surfer',\n",
       " 'celebr',\n",
       " 'cessat',\n",
       " 'war',\n",
       " 'north',\n",
       " 'beach',\n",
       " 'hawaiian',\n",
       " 'island',\n",
       " 'attack',\n",
       " 'japanes',\n",
       " 'zero',\n",
       " 'fifteen',\n",
       " 'year',\n",
       " 'celebr',\n",
       " 'time',\n",
       " 'countri',\n",
       " 'fought',\n",
       " 'cold',\n",
       " 'war',\n",
       " 'instead',\n",
       " 'hot',\n",
       " 'war',\n",
       " 'russian',\n",
       " 'socialist',\n",
       " 'surf',\n",
       " 'ultim',\n",
       " 'narciss',\n",
       " 'danger',\n",
       " 'slightli',\n",
       " 'histor',\n",
       " 'suspect',\n",
       " 'alexand',\n",
       " 'great',\n",
       " 'would',\n",
       " 'celebr',\n",
       " 'surf',\n",
       " 'techniqu',\n",
       " 'go',\n",
       " 'conquer',\n",
       " 'dozen',\n",
       " 'countri',\n",
       " 'get',\n",
       " 'favor',\n",
       " 'press',\n",
       " 'receiv',\n",
       " 'movi',\n",
       " 'militari',\n",
       " 'hero',\n",
       " 'gun',\n",
       " 'beach',\n",
       " 'head',\n",
       " 'surfer',\n",
       " 'conquer',\n",
       " 'beer',\n",
       " 'stand',\n",
       " 'surfboard',\n",
       " 'shop',\n",
       " 'problem',\n",
       " 'peac',\n",
       " 'desper',\n",
       " 'joy',\n",
       " 'exhal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Bag-of-Words features\n",
    "\n",
    "For the model we will be implementing, rather than using the reviews directly, we are going to transform each review into a Bag-of-Words feature representation. Keep in mind that 'in the wild' we will only have access to the training set so our transformer can only use the training set to construct a representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification using XGBoost\n",
    "\n",
    "### Writing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
    "data_dir = '../data/sentiment_update'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Training / Validation files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'sentiment-update'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the XGBoost model\n",
    "\n",
    "- Model Artifacts\n",
    "- Training Code (Container)\n",
    "- Inference Code (Container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Our current execution role is require when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a SageMaker estimator object for our model.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# And then set the algorithm specific parameters.\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-30 15:45:20 Starting - Starting the training job...\n",
      "2019-12-30 15:45:21 Starting - Launching requested ML instances...\n",
      "2019-12-30 15:46:19 Starting - Preparing the instances for training......\n",
      "2019-12-30 15:47:12 Downloading - Downloading input data...\n",
      "2019-12-30 15:47:33 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:47:54:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:47:54:INFO] File size need to be processed in the node: 238.47mb. Available memory size in the node: 8523.48mb\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:47:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:47:54] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[15:47:55] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:47:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:47:55] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[15:47:57] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[15:48:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.295133#011validation-error:0.2992\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[15:48:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.281067#011validation-error:0.283\u001b[0m\n",
      "\u001b[34m[15:48:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.271333#011validation-error:0.2771\u001b[0m\n",
      "\u001b[34m[15:48:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.269667#011validation-error:0.2762\u001b[0m\n",
      "\u001b[34m[15:48:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.259933#011validation-error:0.2655\u001b[0m\n",
      "\n",
      "2019-12-30 15:48:02 Training - Training image download completed. Training in progress.\u001b[34m[15:48:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.243933#011validation-error:0.2548\u001b[0m\n",
      "\u001b[34m[15:48:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.2476#011validation-error:0.2564\u001b[0m\n",
      "\u001b[34m[15:48:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.236133#011validation-error:0.2488\u001b[0m\n",
      "\u001b[34m[15:48:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.2294#011validation-error:0.2429\u001b[0m\n",
      "\u001b[34m[15:48:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.2234#011validation-error:0.2391\u001b[0m\n",
      "\u001b[34m[15:48:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.218933#011validation-error:0.2326\u001b[0m\n",
      "\u001b[34m[15:48:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.214067#011validation-error:0.227\u001b[0m\n",
      "\u001b[34m[15:48:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.2098#011validation-error:0.2189\u001b[0m\n",
      "\u001b[34m[15:48:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.205#011validation-error:0.2166\u001b[0m\n",
      "\u001b[34m[15:48:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.203133#011validation-error:0.2124\u001b[0m\n",
      "\u001b[34m[15:48:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.199133#011validation-error:0.2104\u001b[0m\n",
      "\u001b[34m[15:48:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.196867#011validation-error:0.2091\u001b[0m\n",
      "\u001b[34m[15:48:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.192933#011validation-error:0.2068\u001b[0m\n",
      "\u001b[34m[15:48:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.191067#011validation-error:0.2056\u001b[0m\n",
      "\u001b[34m[15:48:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.1876#011validation-error:0.2015\u001b[0m\n",
      "\u001b[34m[15:48:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.185467#011validation-error:0.1996\u001b[0m\n",
      "\u001b[34m[15:48:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.1808#011validation-error:0.1978\u001b[0m\n",
      "\u001b[34m[15:48:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.179733#011validation-error:0.1964\u001b[0m\n",
      "\u001b[34m[15:48:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.177667#011validation-error:0.1958\u001b[0m\n",
      "\u001b[34m[15:48:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.175333#011validation-error:0.1918\u001b[0m\n",
      "\u001b[34m[15:48:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.1748#011validation-error:0.1894\u001b[0m\n",
      "\u001b[34m[15:48:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.1728#011validation-error:0.188\u001b[0m\n",
      "\u001b[34m[15:48:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.1712#011validation-error:0.1868\u001b[0m\n",
      "\u001b[34m[15:48:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.168867#011validation-error:0.1844\u001b[0m\n",
      "\u001b[34m[15:48:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.1668#011validation-error:0.1844\u001b[0m\n",
      "\u001b[34m[15:48:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.164467#011validation-error:0.1838\u001b[0m\n",
      "\u001b[34m[15:48:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.164067#011validation-error:0.1828\u001b[0m\n",
      "\u001b[34m[15:48:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.1622#011validation-error:0.1819\u001b[0m\n",
      "\u001b[34m[15:48:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.159733#011validation-error:0.1806\u001b[0m\n",
      "\u001b[34m[15:48:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.1568#011validation-error:0.1797\u001b[0m\n",
      "\u001b[34m[15:48:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.1558#011validation-error:0.1789\u001b[0m\n",
      "\u001b[34m[15:48:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.154867#011validation-error:0.1777\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[15:48:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.154067#011validation-error:0.1772\u001b[0m\n",
      "\u001b[34m[15:48:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.152867#011validation-error:0.1759\u001b[0m\n",
      "\u001b[34m[15:48:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.150467#011validation-error:0.1751\u001b[0m\n",
      "\u001b[34m[15:48:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.149067#011validation-error:0.1744\u001b[0m\n",
      "\u001b[34m[15:48:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.148667#011validation-error:0.1741\u001b[0m\n",
      "\u001b[34m[15:48:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.147667#011validation-error:0.1727\u001b[0m\n",
      "\u001b[34m[15:48:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.147533#011validation-error:0.1721\u001b[0m\n",
      "\u001b[34m[15:48:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.1456#011validation-error:0.1713\u001b[0m\n",
      "\u001b[34m[15:48:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.1442#011validation-error:0.1708\u001b[0m\n",
      "\u001b[34m[15:48:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.144067#011validation-error:0.171\u001b[0m\n",
      "\u001b[34m[15:49:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.1428#011validation-error:0.169\u001b[0m\n",
      "\u001b[34m[15:49:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.141667#011validation-error:0.17\u001b[0m\n",
      "\u001b[34m[15:49:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.141667#011validation-error:0.1693\u001b[0m\n",
      "\u001b[34m[15:49:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.140533#011validation-error:0.1683\u001b[0m\n",
      "\u001b[34m[15:49:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.139467#011validation-error:0.167\u001b[0m\n",
      "\u001b[34m[15:49:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.139#011validation-error:0.166\u001b[0m\n",
      "\u001b[34m[15:49:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.138467#011validation-error:0.1655\u001b[0m\n",
      "\u001b[34m[15:49:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.137933#011validation-error:0.1637\u001b[0m\n",
      "\u001b[34m[15:49:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.136133#011validation-error:0.1635\u001b[0m\n",
      "\u001b[34m[15:49:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.135933#011validation-error:0.1622\u001b[0m\n",
      "\u001b[34m[15:49:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.134667#011validation-error:0.1619\u001b[0m\n",
      "\u001b[34m[15:49:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.134667#011validation-error:0.1617\u001b[0m\n",
      "\u001b[34m[15:49:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.1348#011validation-error:0.1631\u001b[0m\n",
      "\u001b[34m[15:49:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.133267#011validation-error:0.1619\u001b[0m\n",
      "\u001b[34m[15:49:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.1326#011validation-error:0.1609\u001b[0m\n",
      "\u001b[34m[15:49:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.131867#011validation-error:0.1583\u001b[0m\n",
      "\u001b[34m[15:49:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.130867#011validation-error:0.1572\u001b[0m\n",
      "\u001b[34m[15:49:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.130267#011validation-error:0.1571\u001b[0m\n",
      "\u001b[34m[15:49:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.1298#011validation-error:0.1574\u001b[0m\n",
      "\u001b[34m[15:49:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.128933#011validation-error:0.158\u001b[0m\n",
      "\u001b[34m[15:49:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.127867#011validation-error:0.1574\u001b[0m\n",
      "\u001b[34m[15:49:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.127867#011validation-error:0.1577\u001b[0m\n",
      "\u001b[34m[15:49:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.126733#011validation-error:0.1565\u001b[0m\n",
      "\u001b[34m[15:49:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.126667#011validation-error:0.1569\u001b[0m\n",
      "\u001b[34m[15:49:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.125#011validation-error:0.1565\u001b[0m\n",
      "\u001b[34m[15:49:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.124533#011validation-error:0.1572\u001b[0m\n",
      "\u001b[34m[15:49:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.124267#011validation-error:0.1568\u001b[0m\n",
      "\u001b[34m[15:49:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.124#011validation-error:0.1562\u001b[0m\n",
      "\u001b[34m[15:49:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.1234#011validation-error:0.1551\u001b[0m\n",
      "\u001b[34m[15:49:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.123067#011validation-error:0.1542\u001b[0m\n",
      "\u001b[34m[15:49:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.1224#011validation-error:0.154\u001b[0m\n",
      "\u001b[34m[15:49:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.121667#011validation-error:0.1536\u001b[0m\n",
      "\u001b[34m[15:49:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.1212#011validation-error:0.1527\u001b[0m\n",
      "\u001b[34m[15:49:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.120933#011validation-error:0.1515\u001b[0m\n",
      "\u001b[34m[15:49:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.120533#011validation-error:0.1506\u001b[0m\n",
      "\u001b[34m[15:49:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.120467#011validation-error:0.151\u001b[0m\n",
      "\u001b[34m[15:49:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.119133#011validation-error:0.1508\u001b[0m\n",
      "\u001b[34m[15:49:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.118267#011validation-error:0.1504\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[15:49:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.118133#011validation-error:0.1507\u001b[0m\n",
      "\u001b[34m[15:49:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.117467#011validation-error:0.15\u001b[0m\n",
      "\u001b[34m[15:49:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.116267#011validation-error:0.1498\u001b[0m\n",
      "\u001b[34m[15:49:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.116#011validation-error:0.1501\u001b[0m\n",
      "\u001b[34m[15:49:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.115133#011validation-error:0.1494\u001b[0m\n",
      "\u001b[34m[15:49:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.115333#011validation-error:0.1486\u001b[0m\n",
      "\u001b[34m[15:49:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.115133#011validation-error:0.149\u001b[0m\n",
      "\u001b[34m[15:49:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.1134#011validation-error:0.1486\u001b[0m\n",
      "\u001b[34m[15:49:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.113467#011validation-error:0.1489\u001b[0m\n",
      "\u001b[34m[15:50:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.1132#011validation-error:0.1494\u001b[0m\n",
      "\u001b[34m[15:50:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.112333#011validation-error:0.1495\u001b[0m\n",
      "\u001b[34m[15:50:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.1122#011validation-error:0.149\u001b[0m\n",
      "\u001b[34m[15:50:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.111733#011validation-error:0.1481\u001b[0m\n",
      "\u001b[34m[15:50:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.111067#011validation-error:0.1477\u001b[0m\n",
      "\u001b[34m[15:50:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.110867#011validation-error:0.1469\u001b[0m\n",
      "\u001b[34m[15:50:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.110467#011validation-error:0.1467\u001b[0m\n",
      "\u001b[34m[15:50:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.110067#011validation-error:0.1464\u001b[0m\n",
      "\u001b[34m[15:50:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.110067#011validation-error:0.1464\u001b[0m\n",
      "\u001b[34m[15:50:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.1094#011validation-error:0.1465\u001b[0m\n",
      "\u001b[34m[15:50:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.108467#011validation-error:0.1478\u001b[0m\n",
      "\u001b[34m[15:50:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.1074#011validation-error:0.1479\u001b[0m\n",
      "\u001b[34m[15:50:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.1074#011validation-error:0.1474\u001b[0m\n",
      "\u001b[34m[15:50:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.1074#011validation-error:0.147\u001b[0m\n",
      "\u001b[34m[15:50:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.106933#011validation-error:0.146\u001b[0m\n",
      "\u001b[34m[15:50:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.106267#011validation-error:0.1448\u001b[0m\n",
      "\u001b[34m[15:50:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.105333#011validation-error:0.1448\u001b[0m\n",
      "\u001b[34m[15:50:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.1048#011validation-error:0.1454\u001b[0m\n",
      "\u001b[34m[15:50:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.104#011validation-error:0.1449\u001b[0m\n",
      "\u001b[34m[15:50:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[113]#011train-error:0.103733#011validation-error:0.1445\u001b[0m\n",
      "\u001b[34m[15:50:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[114]#011train-error:0.103267#011validation-error:0.1441\u001b[0m\n",
      "\u001b[34m[15:50:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[115]#011train-error:0.103467#011validation-error:0.1434\u001b[0m\n",
      "\u001b[34m[15:50:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[116]#011train-error:0.102667#011validation-error:0.1434\u001b[0m\n",
      "\u001b[34m[15:50:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[117]#011train-error:0.101867#011validation-error:0.1431\u001b[0m\n",
      "\u001b[34m[15:50:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[118]#011train-error:0.1018#011validation-error:0.1433\u001b[0m\n",
      "\u001b[34m[15:50:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[119]#011train-error:0.102133#011validation-error:0.1429\u001b[0m\n",
      "\u001b[34m[15:50:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[120]#011train-error:0.1012#011validation-error:0.1428\u001b[0m\n",
      "\u001b[34m[15:50:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[121]#011train-error:0.1006#011validation-error:0.1426\u001b[0m\n",
      "\u001b[34m[15:50:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[122]#011train-error:0.100667#011validation-error:0.1411\u001b[0m\n",
      "\u001b[34m[15:50:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[123]#011train-error:0.1004#011validation-error:0.1406\u001b[0m\n",
      "\u001b[34m[15:50:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[124]#011train-error:0.100533#011validation-error:0.1412\u001b[0m\n",
      "\u001b[34m[15:50:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[125]#011train-error:0.100267#011validation-error:0.1401\u001b[0m\n",
      "\u001b[34m[15:50:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[126]#011train-error:0.100067#011validation-error:0.1398\u001b[0m\n",
      "\u001b[34m[15:50:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[127]#011train-error:0.100667#011validation-error:0.1406\u001b[0m\n",
      "\u001b[34m[15:50:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[128]#011train-error:0.100067#011validation-error:0.1407\u001b[0m\n",
      "\u001b[34m[15:50:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[129]#011train-error:0.099467#011validation-error:0.1411\u001b[0m\n",
      "\u001b[34m[15:50:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[130]#011train-error:0.0988#011validation-error:0.1408\u001b[0m\n",
      "\u001b[34m[15:50:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[131]#011train-error:0.098467#011validation-error:0.141\u001b[0m\n",
      "\u001b[34m[15:50:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[132]#011train-error:0.0982#011validation-error:0.1403\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[15:50:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[133]#011train-error:0.097867#011validation-error:0.1401\u001b[0m\n",
      "\u001b[34m[15:50:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[134]#011train-error:0.0976#011validation-error:0.14\u001b[0m\n",
      "\u001b[34m[15:50:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[135]#011train-error:0.097333#011validation-error:0.1404\u001b[0m\n",
      "\u001b[34m[15:50:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[136]#011train-error:0.096933#011validation-error:0.1403\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[126]#011train-error:0.100067#011validation-error:0.1398\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-30 15:51:01 Uploading - Uploading generated training model\n",
      "2019-12-30 15:51:01 Completed - Training job completed\n",
      "Training seconds: 229\n",
      "Billable seconds: 229\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:54:46:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[34m[2019-12-30 15:54:46 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:54:46:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:54:46:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:54:46:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[32m2019-12-30T15:55:10.634:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:12:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:12:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:12:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:12:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:13:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:13:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:13:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:13:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:16:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:16:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:16:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:16:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:17:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:17:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:17:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:17:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:19:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:19:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:22:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:22:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:22:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:22:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:24:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:24:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:25:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:24:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:24:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:25:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:25:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:25:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:26:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:26:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:27:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:27:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:28:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:27:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:27:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:28:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-30:15:55:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:35:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:15:55:35:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:35:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:15:55:35:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/369.2 KiB (3.3 MiB/s) with 1 file(s) remaining\r",
      "Completed 369.2 KiB/369.2 KiB (4.7 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-eu-west-1-873674308518/xgboost-2019-12-30-15-51-34-273/test.csv.out to ../data/sentiment_update/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85632"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Looking at New Data\n",
    "\n",
    "So now we have an XGBoost sentiment analysis model that we believe is working pretty well. As a result, we deployed it and we are using it in some sort of app.\n",
    "\n",
    "However, as we allow users to use our app we periodically record submitted movie reviews so that we can perform some quality control on our deployed model. Once we've accumulated enough reviews we go through them by hand and evaluate whether they are positive or negative (there are many ways you might do this in practice aside from by hand). The reason for doing this is so that we can check to see how well our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import new_data\n",
    "new_X, new_Y = new_data.get_new_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The `new_data` module assumes that the cache created earlier in Step 3 is still stored in `../cache/sentiment_analysis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the current model on new data\n",
    "\n",
    "First, note that the data that has been loaded has already been pre-processed so that each entry in `new_X` is a list of words that have been processed using `nltk`. However, we have not yet constructed the bag of words encoding, which we will do now.\n",
    "\n",
    "First, we use the vocabulary that we constructed earlier using the original training data to construct a `CountVectorizer` which we will use to transform our new data into its bag of words encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer using the previously constructed vocabulary\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary,\n",
    "                             preprocessor=lambda x: x,  # already preprocessed\n",
    "                             tokenizer=lambda x: x)  # already tokenized\n",
    "\n",
    "# Transform our new data set and store the transformed data in the variable new_XV\n",
    "new_XV = vectorizer.transform(new_X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick sanity check, we make sure that the length of each of our bag of words encoded reviews is correct. In particular, it must be the same size as the vocabulary which in our case is `5000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_XV[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've performed the data processing that is required by our model we can save it locally and then upload it to S3 so that we can construct a batch transform job in order to see how well our model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data contained in new_XV locally in the data_dir with the file name new_data.csv\n",
    "pd.DataFrame(new_XV).to_csv(os.path.join(data_dir, 'new_data.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 and save the resulting URI as new_data_location\n",
    "new_data_location = session.upload_data(os.path.join(data_dir, 'new_data.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, once the new data has been uploaded to S3, we create and run the batch transform job to get our model's predictions about the sentiment of the new movie reviews (using the `xgb_transformer` object that was created earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2019-12-30 16:03:58 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:03:58:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:03:58:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:03:58:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:03:58:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2019-12-30T16:04:28.657:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:36:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:36:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:36:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:36:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:04:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:16:04:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Using xgb_transformer, transform the new_data_location data. You may wish to **wait** until\n",
    "#       the batch transform job has finished.\n",
    "xgb_transformer.transform(new_data_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we copy the results of the batch transform job to our local instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/369.4 KiB (4.4 MiB/s) with 1 file(s) remaining\r",
      "Completed 369.4 KiB/369.4 KiB (6.2 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-eu-west-1-873674308518/xgboost-2019-12-30-16-00-48-533/new_data.csv.out to ../data/sentiment_update/new_data.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the results of the batch transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'new_data.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the accuracy of our current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72748"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(new_Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it would appear that *something* has changed since our model is no longer (as) effective at determining the sentiment of a user provided review.\n",
    "\n",
    "In a real life scenario you would check a number of different things to see what exactly is going on. In our case, we are only going to check one and that is whether some aspect of the underlying distribution has changed. In other words, we want to see if the words that appear in our new collection of reviews matches the words that appear in the original training set. Of course, we want to narrow our scope a little bit so we will only look at the `5000` most frequently appearing words in each data set, or in other words, the vocabulary generated by each data set.\n",
    "\n",
    "Before doing that, however, let's take a look at some of the incorrectly classified reviews in the new data set.\n",
    "\n",
    "To start, we will deploy the original XGBoost model. We will then use the deployed model to infer the sentiment of some of the new reviews. This will also serve as a nice excuse to deploy our model so that we can mimic a real life scenario where we have a model that has been deployed and is being used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: xgboost-2019-12-30-15-45-20-409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model that was created earlier. Recall that the object name is 'xgb'.\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose the problem\n",
    "\n",
    "Now that we have our deployed \"production\" model, we can send some of our new data to it and filter out some of the incorrectly classified reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# We need to tell the endpoint what format the data we are sending is in so that SageMaker can perform the serialization.\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful to look at a few different examples of incorrectly classified reviews so we will start by creating a *generator* which we will use to iterate through some of the new reviews and find ones that are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(in_X, in_XV, in_Y):\n",
    "    for idx, smp in enumerate(in_X):\n",
    "        res = round(float(xgb_predictor.predict(in_XV[idx])))\n",
    "        if res != in_Y[idx]:\n",
    "            yield smp, in_Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `gn` is the *generator* which generates samples from the new data set which are not classified correctly. To get the *next* sample we simply call the `next` method on our generator. (Note: The reason we use generators here is so that we don't have to iterate through all of the new reviews, searching for incorrectly classified samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['real', 'charact', 'stori', 'driven', 'drama', 'level', 'shame', 'see', 'tv', 'mo', 'impress', 'right', 'start', 'put', 'sci', 'fi', 'nut', 'like', 'could', 'happen', 'earth', 'fact', 'anoth', 'galaxi', 'make', 'show', 'interest', 'space', 'ship', 'laser', 'gun', 'none', 'yet', 'anyway', 'far', 'seen', 's01', 'e04', 'grip', 'wonder', 'what', 'go', 'happen', 'next', 'mani', 'possibl', 'cast', 'play', 'role', 'pasion', 'eric', 'stoltz', 'especi', 'strong', 'show', 'realli', 'stand', 'alon', 'well', 'matter', 'watch', 'bsg', 'fact', 'quit', 'differ', 'read', 'neg', 'review', 'sci', 'fi', 'geek', 'expect', 'less', 'drama', 'alien', 'ray', 'gun', 'etc', 'would', 'say', 'ignor', 'realli', 'posit', 'start', 'show', 'let', 'hope', 'cann', '1', '2', 'season', 'like', 'normal', 'good', 'show', 'day'], 1)\n"
     ]
    }
   ],
   "source": [
    "# gn = get_sample(new_X, new_XV, new_Y)\n",
    "print(next(gn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at a few examples, maybe we decide to look at the most frequently appearing `5000` words in each data set, the original training data set and the new data set. The reason for looking at this might be that we expect the frequency of use of different words to have changed, maybe there is some new slang that has been introduced or some other artifact of popular culture that has changed the way that people write movie reviews.\n",
    "\n",
    "To do this, we start by fitting a `CountVectorizer` to the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function <lambda> at 0x7efee051c7b8>,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function <lambda> at 0x7efee051c840>, vocabulary=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectorizer = CountVectorizer(max_features=5000,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "new_vectorizer.fit(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this new `CountVectorizor` object, we can check to see if the corresponding vocabulary has changed between the two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vocabulary = set(vocabulary.keys())\n",
    "new_vocabulary = set(new_vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the words that were in the original vocabulary but not in the new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ghetto', 'playboy', '21st', 'reincarn', 'weari', 'victorian', 'spill'}\n"
     ]
    }
   ],
   "source": [
    "print(original_vocabulary - new_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly, we can look at the words that are in the new vocabulary but which were not in the original vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banana', 'optimist', 'masterson', 'dubiou', 'orchestr', 'sophi', 'omin'}\n"
     ]
    }
   ],
   "source": [
    "print(new_vocabulary - original_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words themselves don't tell us much, however if one of these words occured with a large frequency, that might tell us something. In particular, we wouldn't really expect any of the words above to appear with too much frequency.\n",
    "\n",
    "**Question** What exactly is going on here. Not only what (if any) words appear with a larger than expected frequency but also, what does this mean? What has changed about the world that our original model no longer takes into account?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana, 5042\n",
      "optimist, 62\n",
      "masterson, 62\n",
      "dubiou, 62\n",
      "orchestr, 62\n",
      "sophi, 62\n",
      "omin, 62\n"
     ]
    }
   ],
   "source": [
    "# Check frequencies of new vocab\n",
    "new_vocab_flat = [word for review in new_X for word in review]\n",
    "\n",
    "from collections import Counter\n",
    "count_dict = Counter(new_vocab_flat)\n",
    "\n",
    "for word in (new_vocabulary - original_vocabulary):\n",
    "    print(word+\", \"+str(count_dict[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The word banana has been introduced quite often ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a new model\n",
    "\n",
    "Supposing that we believe something has changed about the underlying distribution of the words that our reviews are made up of, we need to create a new model. This way our new model will take into account whatever it is that has changed.\n",
    "\n",
    "To begin with, we will use the new vocabulary to create a bag of words encoding of the new data. We will then use this data to train a new XGBoost model.\n",
    "\n",
    "**NOTE:** Because we believe that the underlying distribution of words has changed it should follow that the original vocabulary that we used to construct a bag of words encoding of the reviews is no longer valid. This means that we need to be careful with our data. If we send an bag of words encoded review using the *original* vocabulary we should not expect any sort of meaningful results.\n",
    "\n",
    "In particular, this means that if we had deployed our XGBoost model like we did in the Web App notebook then we would need to implement this vocabulary change in the Lambda function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_XV = new_vectorizer.transform(new_X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check result\n",
    "len(new_XV[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our newly encoded, newly collected data, we can split it up into a training and validation set so that we can train a new XGBoost model. As usual, we first split up the data, then save it locally and then upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "new_val_X = pd.DataFrame(new_XV[:10000])\n",
    "new_train_X = pd.DataFrame(new_XV[10000:])\n",
    "\n",
    "new_val_y = pd.DataFrame(new_Y[:10000])\n",
    "new_train_y = pd.DataFrame(new_Y[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save some memory we will effectively delete the `new_X` variable. Remember that this contained a list of reviews and each review was a list of words. Note that once this cell has been executed you will need to read the new data in again if you want to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we save the new training and validation sets locally. Note that we overwrite the training and validation sets used earlier. This is mostly because the amount of space that we have available on our notebook instance is limited. Of course, you can increase this if you'd like but to do so may increase the cost of running the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(new_XV).to_csv(os.path.join(data_dir, 'new_data.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([new_val_y, new_val_X], axis=1).to_csv(os.path.join(data_dir, 'new_validation.csv'), header=False, index=False)\n",
    "pd.concat([new_train_y, new_train_X], axis=1).to_csv(os.path.join(data_dir, 'new_train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've saved our data to the local instance, we can safely delete the variables to save on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_y = new_val_X = new_train_y = new_train_X = new_XV = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we make sure to upload the new training and validation sets to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Upload the new data and the new validation.csv and train.csv files in the data_dir directory to S3.\n",
    "new_data_location = session.upload_data(os.path.join(data_dir, 'new_data.csv'), key_prefix=prefix)\n",
    "new_val_location = session.upload_data(os.path.join(data_dir, 'new_validation.csv'), key_prefix=prefix)\n",
    "new_train_location = session.upload_data(os.path.join(data_dir, 'new_train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our new training data has been uploaded to S3, we can create a new XGBoost model that will take into account the changes that have occured in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker estimator object for our model.\n",
    "new_xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# Set the algorithm specific parameters.\n",
    "new_xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been created, we can train it with our new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create s3 input objects\n",
    "s3_new_input_train = sagemaker.s3_input(s3_data=new_train_location, content_type='csv')\n",
    "s3_new_input_validation = sagemaker.s3_input(s3_data=new_val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-30 16:51:20 Starting - Starting the training job...\n",
      "2019-12-30 16:51:22 Starting - Launching requested ML instances...\n",
      "2019-12-30 16:52:21 Starting - Preparing the instances for training.........\n",
      "2019-12-30 16:53:50 Downloading - Downloading input data\n",
      "2019-12-30 16:53:50 Training - Downloading the training image...\n",
      "2019-12-30 16:54:11 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:54:12:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:54:12:INFO] File size need to be processed in the node: 238.47mb. Available memory size in the node: 8516.01mb\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:54:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[16:54:12] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[16:54:14] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2019-12-30:16:54:14:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[16:54:14] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[16:54:15] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[16:54:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.311#011validation-error:0.3133\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[16:54:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.2958#011validation-error:0.2975\u001b[0m\n",
      "\u001b[34m[16:54:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.286#011validation-error:0.29\u001b[0m\n",
      "\u001b[34m[16:54:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.271667#011validation-error:0.2799\u001b[0m\n",
      "\u001b[34m[16:54:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.2752#011validation-error:0.2803\u001b[0m\n",
      "\u001b[34m[16:54:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.264333#011validation-error:0.2725\u001b[0m\n",
      "\u001b[34m[16:54:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.2636#011validation-error:0.271\u001b[0m\n",
      "\u001b[34m[16:54:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.2598#011validation-error:0.2675\u001b[0m\n",
      "\u001b[34m[16:54:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.247667#011validation-error:0.257\u001b[0m\n",
      "\u001b[34m[16:54:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.237867#011validation-error:0.2526\u001b[0m\n",
      "\u001b[34m[16:54:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.232467#011validation-error:0.2469\u001b[0m\n",
      "\u001b[34m[16:54:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.229867#011validation-error:0.242\u001b[0m\n",
      "\u001b[34m[16:54:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.227333#011validation-error:0.2402\u001b[0m\n",
      "\u001b[34m[16:54:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.224667#011validation-error:0.238\u001b[0m\n",
      "\u001b[34m[16:54:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.2206#011validation-error:0.2352\u001b[0m\n",
      "\u001b[34m[16:54:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.214267#011validation-error:0.2312\u001b[0m\n",
      "\u001b[34m[16:54:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.210867#011validation-error:0.2282\u001b[0m\n",
      "\u001b[34m[16:54:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.207867#011validation-error:0.2268\u001b[0m\n",
      "\u001b[34m[16:54:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.2032#011validation-error:0.2251\u001b[0m\n",
      "\u001b[34m[16:54:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.202533#011validation-error:0.2223\u001b[0m\n",
      "\u001b[34m[16:54:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.199133#011validation-error:0.2191\u001b[0m\n",
      "\u001b[34m[16:54:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.195733#011validation-error:0.218\u001b[0m\n",
      "\u001b[34m[16:54:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.1914#011validation-error:0.2153\u001b[0m\n",
      "\u001b[34m[16:54:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.1896#011validation-error:0.2135\u001b[0m\n",
      "\u001b[34m[16:54:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.1878#011validation-error:0.2113\u001b[0m\n",
      "\u001b[34m[16:54:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.184533#011validation-error:0.2082\u001b[0m\n",
      "\u001b[34m[16:54:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.183733#011validation-error:0.2068\u001b[0m\n",
      "\u001b[34m[16:54:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.181667#011validation-error:0.2052\u001b[0m\n",
      "\u001b[34m[16:54:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.180667#011validation-error:0.2033\u001b[0m\n",
      "\u001b[34m[16:54:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.178333#011validation-error:0.2021\u001b[0m\n",
      "\u001b[34m[16:54:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.175733#011validation-error:0.2009\u001b[0m\n",
      "\u001b[34m[16:54:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.173667#011validation-error:0.1983\u001b[0m\n",
      "\u001b[34m[16:55:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.1734#011validation-error:0.1976\u001b[0m\n",
      "\u001b[34m[16:55:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.170733#011validation-error:0.1962\u001b[0m\n",
      "\u001b[34m[16:55:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.170267#011validation-error:0.1957\u001b[0m\n",
      "\u001b[34m[16:55:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.167467#011validation-error:0.1926\u001b[0m\n",
      "\u001b[34m[16:55:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.166733#011validation-error:0.1925\u001b[0m\n",
      "\u001b[34m[16:55:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.1658#011validation-error:0.1909\u001b[0m\n",
      "\u001b[34m[16:55:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.164067#011validation-error:0.1906\u001b[0m\n",
      "\u001b[34m[16:55:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.163#011validation-error:0.1899\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[16:55:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.161133#011validation-error:0.1881\u001b[0m\n",
      "\u001b[34m[16:55:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.161133#011validation-error:0.1867\u001b[0m\n",
      "\u001b[34m[16:55:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.1598#011validation-error:0.1867\u001b[0m\n",
      "\u001b[34m[16:55:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.158#011validation-error:0.1867\u001b[0m\n",
      "\u001b[34m[16:55:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.1572#011validation-error:0.1869\u001b[0m\n",
      "\u001b[34m[16:55:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.156133#011validation-error:0.1854\u001b[0m\n",
      "\u001b[34m[16:55:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.1554#011validation-error:0.1851\u001b[0m\n",
      "\u001b[34m[16:55:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.154467#011validation-error:0.1855\u001b[0m\n",
      "\u001b[34m[16:55:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.1534#011validation-error:0.1842\u001b[0m\n",
      "\u001b[34m[16:55:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.1528#011validation-error:0.1827\u001b[0m\n",
      "\u001b[34m[16:55:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.1524#011validation-error:0.1829\u001b[0m\n",
      "\u001b[34m[16:55:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.151#011validation-error:0.1834\u001b[0m\n",
      "\u001b[34m[16:55:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.1502#011validation-error:0.1826\u001b[0m\n",
      "\u001b[34m[16:55:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.1498#011validation-error:0.1814\u001b[0m\n",
      "\u001b[34m[16:55:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.148267#011validation-error:0.1809\u001b[0m\n",
      "\u001b[34m[16:55:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.147733#011validation-error:0.1801\u001b[0m\n",
      "\u001b[34m[16:55:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.1472#011validation-error:0.1792\u001b[0m\n",
      "\u001b[34m[16:55:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.147133#011validation-error:0.1777\u001b[0m\n",
      "\u001b[34m[16:55:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.146267#011validation-error:0.1781\u001b[0m\n",
      "\u001b[34m[16:55:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.145333#011validation-error:0.1772\u001b[0m\n",
      "\u001b[34m[16:55:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.144533#011validation-error:0.1767\u001b[0m\n",
      "\u001b[34m[16:55:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.143733#011validation-error:0.1767\u001b[0m\n",
      "\u001b[34m[16:55:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.141867#011validation-error:0.1767\u001b[0m\n",
      "\u001b[34m[16:55:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.141467#011validation-error:0.1773\u001b[0m\n",
      "\u001b[34m[16:55:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.141533#011validation-error:0.1786\u001b[0m\n",
      "\u001b[34m[16:55:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.141533#011validation-error:0.1785\u001b[0m\n",
      "\u001b[34m[16:55:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.140533#011validation-error:0.1765\u001b[0m\n",
      "\u001b[34m[16:55:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.139133#011validation-error:0.1775\u001b[0m\n",
      "\u001b[34m[16:55:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.1384#011validation-error:0.1772\u001b[0m\n",
      "\u001b[34m[16:55:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.1378#011validation-error:0.1773\u001b[0m\n",
      "\u001b[34m[16:55:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.136333#011validation-error:0.1764\u001b[0m\n",
      "\u001b[34m[16:55:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.135867#011validation-error:0.1758\u001b[0m\n",
      "\u001b[34m[16:55:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.1356#011validation-error:0.1755\u001b[0m\n",
      "\u001b[34m[16:55:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.135#011validation-error:0.1753\u001b[0m\n",
      "\u001b[34m[16:55:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.134733#011validation-error:0.1749\u001b[0m\n",
      "\u001b[34m[16:55:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.134333#011validation-error:0.1749\u001b[0m\n",
      "\u001b[34m[16:55:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.134333#011validation-error:0.1753\u001b[0m\n",
      "\u001b[34m[16:55:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.134467#011validation-error:0.1753\u001b[0m\n",
      "\u001b[34m[16:55:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.133733#011validation-error:0.1757\u001b[0m\n",
      "\u001b[34m[16:55:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.1332#011validation-error:0.1753\u001b[0m\n",
      "\u001b[34m[16:56:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.132267#011validation-error:0.1751\u001b[0m\n",
      "\u001b[34m[16:56:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.131867#011validation-error:0.1747\u001b[0m\n",
      "\u001b[34m[16:56:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.1314#011validation-error:0.1752\u001b[0m\n",
      "\u001b[34m[16:56:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.1308#011validation-error:0.1756\u001b[0m\n",
      "\u001b[34m[16:56:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.131133#011validation-error:0.1747\u001b[0m\n",
      "\u001b[34m[16:56:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.1298#011validation-error:0.175\u001b[0m\n",
      "\u001b[34m[16:56:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.129#011validation-error:0.1739\u001b[0m\n",
      "\u001b[34m[16:56:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.1284#011validation-error:0.1732\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[16:56:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.128067#011validation-error:0.1739\u001b[0m\n",
      "\u001b[34m[16:56:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.1274#011validation-error:0.1743\u001b[0m\n",
      "\u001b[34m[16:56:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.127133#011validation-error:0.1737\u001b[0m\n",
      "\u001b[34m[16:56:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.126733#011validation-error:0.1728\u001b[0m\n",
      "\u001b[34m[16:56:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.126333#011validation-error:0.1724\u001b[0m\n",
      "\u001b[34m[16:56:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.1262#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[16:56:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.1262#011validation-error:0.1719\u001b[0m\n",
      "\u001b[34m[16:56:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.125333#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[16:56:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.125067#011validation-error:0.1728\u001b[0m\n",
      "\u001b[34m[16:56:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.125067#011validation-error:0.1735\u001b[0m\n",
      "\u001b[34m[16:56:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.125#011validation-error:0.1734\u001b[0m\n",
      "\u001b[34m[16:56:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.124867#011validation-error:0.1721\u001b[0m\n",
      "\u001b[34m[16:56:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.1248#011validation-error:0.1719\u001b[0m\n",
      "\u001b[34m[16:56:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.124267#011validation-error:0.1718\u001b[0m\n",
      "\u001b[34m[16:56:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.124#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[16:56:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.123133#011validation-error:0.1716\u001b[0m\n",
      "\u001b[34m[16:56:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.122333#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[16:56:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.122267#011validation-error:0.1723\u001b[0m\n",
      "\u001b[34m[16:56:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.1224#011validation-error:0.1725\u001b[0m\n",
      "\u001b[34m[16:56:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.1222#011validation-error:0.1728\u001b[0m\n",
      "\u001b[34m[16:56:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.1218#011validation-error:0.1729\u001b[0m\n",
      "\u001b[34m[16:56:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.121267#011validation-error:0.1715\u001b[0m\n",
      "\u001b[34m[16:56:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.1208#011validation-error:0.1711\u001b[0m\n",
      "\u001b[34m[16:56:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.121067#011validation-error:0.1715\u001b[0m\n",
      "\u001b[34m[16:56:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.120933#011validation-error:0.1716\u001b[0m\n",
      "\u001b[34m[16:56:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[113]#011train-error:0.119933#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[16:56:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[114]#011train-error:0.1196#011validation-error:0.1719\u001b[0m\n",
      "\u001b[34m[16:56:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[115]#011train-error:0.119733#011validation-error:0.1717\u001b[0m\n",
      "\u001b[34m[16:56:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[116]#011train-error:0.119267#011validation-error:0.1718\u001b[0m\n",
      "\u001b[34m[16:56:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[117]#011train-error:0.118733#011validation-error:0.172\u001b[0m\n",
      "\u001b[34m[16:56:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[118]#011train-error:0.1186#011validation-error:0.1735\u001b[0m\n",
      "\u001b[34m[16:56:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[119]#011train-error:0.118267#011validation-error:0.1735\u001b[0m\n",
      "\u001b[34m[16:56:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[120]#011train-error:0.118667#011validation-error:0.1736\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.1208#011validation-error:0.1711\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-30 16:57:01 Uploading - Uploading generated training model\n",
      "2019-12-30 16:57:01 Completed - Training job completed\n",
      "Training seconds: 210\n",
      "Billable seconds: 210\n"
     ]
    }
   ],
   "source": [
    "# 'Fit' your new model.\n",
    "new_xgb.fit({'train': s3_new_input_train, 'validation': s3_new_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the new model\n",
    "\n",
    "So now we have a new XGBoost model that we believe more accurately represents the state of the world at this time, at least in how it relates to the sentiment analysis problem that we are working on. The next step is to double check that our model is performing reasonably. To do this, we will first test our model on the new data.\n",
    "\n",
    "**Note:** In practice this is a pretty bad idea. We already trained our model on the new data, so testing it shouldn't really tell us much. In fact, this is sort of a textbook example of leakage. We are only doing it here so that we have a numerical baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer object from the new_xgb model\n",
    "new_xgb_transformer = new_xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we test our model on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:43:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:00:43 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:43:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:43:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:43:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2019-12-30T17:00:48.033:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:00:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:08:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:08:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:08:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:08:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:08:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:08:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-30:17:01:12:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:13:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:01:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:12:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:13:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:01:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the new_data_location data\n",
    "new_xgb_transformer.transform(new_data_location, content_type='text/csv', split_type='Line')\n",
    "new_xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the results to our local instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/366.6 KiB (2.1 MiB/s) with 1 file(s) remaining\r",
      "Completed 366.6 KiB/366.6 KiB (2.9 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-eu-west-1-873674308518/xgboost-2019-12-30-16-57-34-398/new_data.csv.out to ../data/sentiment_update/new_data.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $new_xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how well the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'new_data.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85908"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(new_Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, since we trained the model on this data, our model performs pretty well. So, we have reason to believe that our new XGBoost model is a \"better\" model.\n",
    "\n",
    "However, before we start changing our deployed model, we should first make sure that our new model isn't too different. In other words, if our new model performed really poorly on the original test data then this might be an indication that something else has gone wrong.\n",
    "\n",
    "To start with, since we got rid of the variable that stored the original test reviews, we will read them in again from the cache that we created in Step 3. Note that we need to make sure that we read in the original test data after it has been pre-processed with `nltk` but before it has been bag of words encoded. This is because we need to use the new vocabulary instead of the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "cache_data = None\n",
    "with open(os.path.join(cache_dir, \"preprocessed_data.pkl\"), \"rb\") as f:\n",
    "            cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", \"preprocessed_data.pkl\")\n",
    "            \n",
    "test_X = cache_data['words_test']\n",
    "test_Y = cache_data['labels_test']\n",
    "\n",
    "# Here we set cache_data to None so that it doesn't occupy memory\n",
    "cache_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've loaded the original test reviews, we need to create a bag of words encoding of them using the new vocabulary that we created, based on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new_vectorizer object that you created earlier to transform the test_X data.\n",
    "test_X = new_vectorizer.transform(test_X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have correctly encoded the original test data, we can write it to the local instance, upload it to S3 and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2019-12-30 17:07:16 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:16:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:16:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:16:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:16:INFO] Model loaded successfully for worker : 42\u001b[0m\n",
      "\u001b[32m2019-12-30T17:07:35.369:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:07:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:08:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:08:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:08:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:08:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:08:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2019-12-30:17:08:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:07:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:08:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:08:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:08:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:08:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:08:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2019-12-30:17:08:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "new_xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/367.0 KiB (3.8 MiB/s) with 1 file(s) remaining\r",
      "Completed 367.0 KiB/367.0 KiB (5.1 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-eu-west-1-873674308518/xgboost-2019-12-30-17-04-14-420/test.csv.out to ../data/sentiment_update/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $new_xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83992"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that our new XGBoost model is performing quite well on the old test data. This gives us some indication that our new model should be put into production and replace our original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Updating the Model\n",
    "\n",
    "So we have a new model that we'd like to use instead of one that is already deployed. Furthermore, we are assuming that the model that is already deployed is being used in some sort of application. As a result, what we want to do is update the existing endpoint so that it uses our new model.\n",
    "\n",
    "Of course, to do this we need to create an endpoint configuration for our newly created model.\n",
    "\n",
    "First, note that we can access the name of the model that we created above using the `model_name` property of the transformer. The reason for this is that in order for the transformer to create a batch transform job it needs to first create the model object inside of SageMaker. Since we've sort of already done this we should take advantage of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2019-12-30-16-51-20-594'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_xgb_transformer.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an endpoint configuration using the low level approach of creating the dictionary object which describes the endpoint configuration we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Give endpoint a unique configuration a name\n",
    "new_xgb_endpoint_config_name = \"sentiment-update-xgboost-endpoint-config-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Using the SageMaker Client, construct the endpoint configuration\n",
    "new_xgb_endpoint_config_info = session.sagemaker_client.create_endpoint_config(\n",
    "                            EndpointConfigName = new_xgb_endpoint_config_name,\n",
    "                            ProductionVariants = [{\n",
    "                                \"InstanceType\": \"ml.m4.xlarge\",\n",
    "                                \"InitialVariantWeight\": 1,\n",
    "                                \"InitialInstanceCount\": 1,\n",
    "                                \"ModelName\": new_xgb_transformer.model_name,\n",
    "                                \"VariantName\": \"XGB-Model\"\n",
    "                            }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint configuration has been constructed, it is a straightforward matter to ask SageMaker to update the existing endpoint so that it uses the new endpoint configuration.\n",
    "\n",
    "Of note here is that SageMaker does this in such a way that there is no downtime. Essentially, SageMaker deploys the new model and then updates the original endpoint so that it points to the newly deployed model. After that, the original model is shut down. This way, whatever app is using our endpoint won't notice that we've changed the model that is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:eu-west-1:873674308518:endpoint/xgboost-2019-12-30-15-45-20-409',\n",
       " 'ResponseMetadata': {'RequestId': '0f8ce405-8c89-4730-be37-0215f0c7a530',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0f8ce405-8c89-4730-be37-0215f0c7a530',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Mon, 30 Dec 2019 17:08:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the xgb_predictor.endpoint so that it uses new_xgb_endpoint_config_name.\n",
    "session.sagemaker_client.update_endpoint(EndpointName=xgb_predictor.endpoint, \n",
    "                                         EndpointConfigName=new_xgb_endpoint_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as is generally the case with SageMaker requests, this is being done in the background so if we want to wait for it to complete we need to call the appropriate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'xgboost-2019-12-30-15-45-20-409',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:eu-west-1:873674308518:endpoint/xgboost-2019-12-30-15-45-20-409',\n",
       " 'EndpointConfigName': 'sentiment-update-xgboost-endpoint-config-2019-12-30-17-08-28',\n",
       " 'ProductionVariants': [{'VariantName': 'XGB-Model',\n",
       "   'DeployedImages': [{'SpecifiedImage': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:1',\n",
       "     'ResolvedImage': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost@sha256:5fe3063b6797a14fec0da6c3c6d6b8cb484773c595864e91b85fa1e6168d3a38',\n",
       "     'ResolutionTime': datetime.datetime(2019, 12, 30, 17, 8, 33, 45000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2019, 12, 30, 16, 5, 33, 141000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2019, 12, 30, 17, 16, 45, 595000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '8ccdf615-a51a-4d44-b72c-0a76651d0a08',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8ccdf615-a51a-4d44-b72c-0a76651d0a08',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '742',\n",
       "   'date': 'Mon, 30 Dec 2019 17:16:46 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.wait_for_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Delete the Endpoint\n",
    "\n",
    "Of course, since we are done with the deployed endpoint we need to make sure to shut it down, otherwise we will continue to be charged for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm $data_dir/*\n",
    "!rmdir $data_dir\n",
    "!rm $cache_dir/*\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
